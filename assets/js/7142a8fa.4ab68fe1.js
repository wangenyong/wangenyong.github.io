"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5821],{3905:(e,a,n)=>{n.d(a,{Zo:()=>d,kt:()=>k});var t=n(7294);function l(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function r(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function s(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?r(Object(n),!0).forEach((function(a){l(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function i(e,a){if(null==e)return{};var n,t,l=function(e,a){if(null==e)return{};var n,t,l={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||(l[n]=e[n]);return l}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var o=t.createContext({}),m=function(e){var a=t.useContext(o),n=a;return e&&(n="function"==typeof e?e(a):s(s({},a),e)),n},d=function(e){var a=m(e.components);return t.createElement(o.Provider,{value:a},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},c=t.forwardRef((function(e,a){var n=e.components,l=e.mdxType,r=e.originalType,o=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),u=m(n),c=l,k=u["".concat(o,".").concat(c)]||u[c]||p[c]||r;return n?t.createElement(k,s(s({ref:a},d),{},{components:n})):t.createElement(k,s({ref:a},d))}));function k(e,a){var n=arguments,l=a&&a.mdxType;if("string"==typeof e||l){var r=n.length,s=new Array(r);s[0]=c;var i={};for(var o in a)hasOwnProperty.call(a,o)&&(i[o]=a[o]);i.originalType=e,i[u]="string"==typeof e?e:l,s[1]=i;for(var m=2;m<r;m++)s[m]=n[m];return t.createElement.apply(null,s)}return t.createElement.apply(null,n)}c.displayName="MDXCreateElement"},728:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>o,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>m});var t=n(7462),l=(n(7294),n(3905));const r={sidebar_position:3,id:"Spark-SQL-Basic",title:"spark SQL \u57fa\u672c\u4f7f\u7528"},s=void 0,i={unversionedId:"bigdata/Spark-SQL-Basic",id:"bigdata/Spark-SQL-Basic",title:"spark SQL \u57fa\u672c\u4f7f\u7528",description:"\u521b\u5efa DataFrame",source:"@site/docs/bigdata/spark-SQL-Basic.md",sourceDirName:"bigdata",slug:"/bigdata/Spark-SQL-Basic",permalink:"/docs/bigdata/Spark-SQL-Basic",draft:!1,editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/bigdata/spark-SQL-Basic.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,id:"Spark-SQL-Basic",title:"spark SQL \u57fa\u672c\u4f7f\u7528"},sidebar:"tutorialSidebar",previous:{title:"spark RDD",permalink:"/docs/bigdata/spark-RDD"},next:{title:"spark SQL \u6761\u4ef6\u8fc7\u6ee4",permalink:"/docs/bigdata/Spark-SQL-Filter"}},o={},m=[{value:"\u521b\u5efa DataFrame",id:"\u521b\u5efa-dataframe",level:2},{value:"\u6570\u636e\u51c6\u5907",id:"\u6570\u636e\u51c6\u5907",level:3},{value:"\u901a\u8fc7 RDD \u521b\u5efa DataFrame",id:"\u901a\u8fc7-rdd-\u521b\u5efa-dataframe",level:3},{value:"toDF()",id:"todf",level:4},{value:"createDataFrame()",id:"createdataframe",level:4},{value:"\u901a\u8fc7 List \u548c Seq \u96c6\u5408\u521b\u5efa DataFrame",id:"\u901a\u8fc7-list-\u548c-seq-\u96c6\u5408\u521b\u5efa-dataframe",level:3},{value:"toDF()",id:"todf-1",level:4},{value:"createDataFrame()",id:"createdataframe-1",level:4},{value:"\u901a\u8fc7 CSV \u521b\u5efa DataFrame",id:"\u901a\u8fc7-csv-\u521b\u5efa-dataframe",level:3},{value:"\u901a\u8fc7 TXT \u521b\u5efa DataFrame",id:"\u901a\u8fc7-txt-\u521b\u5efa-dataframe",level:3},{value:"\u901a\u8fc7 JSON \u521b\u5efa DataFrame",id:"\u901a\u8fc7-json-\u521b\u5efa-dataframe",level:3},{value:"\u901a\u8fc7 XML \u521b\u5efa DataFrame",id:"\u901a\u8fc7-xml-\u521b\u5efa-dataframe",level:3},{value:"\u901a\u8fc7 MySQL \u521b\u5efa DataFrame",id:"\u901a\u8fc7-mysql-\u521b\u5efa-dataframe",level:3},{value:"\u9009\u62e9\u5217",id:"\u9009\u62e9\u5217",level:2},{value:"\u6570\u636e\u51c6\u5907",id:"\u6570\u636e\u51c6\u5907-1",level:3},{value:"\u9009\u62e9\u5355\u5217\u6216\u591a\u5217",id:"\u9009\u62e9\u5355\u5217\u6216\u591a\u5217",level:3},{value:"\u9009\u62e9\u6240\u6709\u5217",id:"\u9009\u62e9\u6240\u6709\u5217",level:3},{value:"\u6839\u636e\u5217\u8868\u9009\u62e9\u5217",id:"\u6839\u636e\u5217\u8868\u9009\u62e9\u5217",level:3},{value:"\u9009\u62e9\u524d N \u5217",id:"\u9009\u62e9\u524d-n-\u5217",level:3},{value:"\u6839\u636e\u4f4d\u7f6e\u6216\u7d22\u5f15\u9009\u62e9\u5217",id:"\u6839\u636e\u4f4d\u7f6e\u6216\u7d22\u5f15\u9009\u62e9\u5217",level:3},{value:"\u901a\u8fc7\u6b63\u5219\u9009\u62e9\u5217",id:"\u901a\u8fc7\u6b63\u5219\u9009\u62e9\u5217",level:3},{value:"\u6839\u636e starts \u6216\u8005 ends \u9009\u62e9\u5217",id:"\u6839\u636e-starts-\u6216\u8005-ends-\u9009\u62e9\u5217",level:3},{value:"\u9009\u62e9\u591a\u5c42\u7ea7\u5217",id:"\u9009\u62e9\u591a\u5c42\u7ea7\u5217",level:3},{value:"\u521b\u5efa\u6216\u4fee\u6539\u5217",id:"\u521b\u5efa\u6216\u4fee\u6539\u5217",level:2},{value:"\u6570\u636e\u51c6\u5907",id:"\u6570\u636e\u51c6\u5907-2",level:3},{value:"\u521b\u5efa\u65b0\u5217",id:"\u521b\u5efa\u65b0\u5217",level:3},{value:"\u4fee\u6539\u5217\u7684\u503c",id:"\u4fee\u6539\u5217\u7684\u503c",level:3},{value:"\u6839\u636e\u73b0\u6709\u5217\u589e\u52a0\u65b0\u5217",id:"\u6839\u636e\u73b0\u6709\u5217\u589e\u52a0\u65b0\u5217",level:3},{value:"\u4fee\u6539\u5217\u7c7b\u578b",id:"\u4fee\u6539\u5217\u7c7b\u578b",level:3},{value:"\u4fee\u6539\u591a\u5217",id:"\u4fee\u6539\u591a\u5217",level:3},{value:"\u5217\u91cd\u547d\u540d",id:"\u5217\u91cd\u547d\u540d",level:3},{value:"\u5220\u9664\u5217",id:"\u5220\u9664\u5217",level:3},{value:"\u62c6\u5206\u5217\u4e3a\u591a\u5217",id:"\u62c6\u5206\u5217\u4e3a\u591a\u5217",level:3},{value:"\u5217\u91cd\u547d\u540d",id:"\u5217\u91cd\u547d\u540d-1",level:2},{value:"\u6570\u636e\u51c6\u5907",id:"\u6570\u636e\u51c6\u5907-3",level:3},{value:"\u57fa\u672c\u4f7f\u7528",id:"\u57fa\u672c\u4f7f\u7528",level:3},{value:"\u91cd\u547d\u540d\u591a\u5217",id:"\u91cd\u547d\u540d\u591a\u5217",level:3},{value:"\u4f7f\u7528 Spark StructType \u91cd\u547d\u540d",id:"\u4f7f\u7528-spark-structtype-\u91cd\u547d\u540d",level:3},{value:"\u4f7f\u7528 Select \u91cd\u547d\u540d",id:"\u4f7f\u7528-select-\u91cd\u547d\u540d",level:3},{value:"\u4f7f\u7528 withColumn \u91cd\u547d\u540d",id:"\u4f7f\u7528-withcolumn-\u91cd\u547d\u540d",level:3},{value:"\u4f7f\u7528 col() \u6279\u91cf\u4fee\u6539\u5217\u540d",id:"\u4f7f\u7528-col-\u6279\u91cf\u4fee\u6539\u5217\u540d",level:3},{value:"\u5220\u9664\u5217",id:"\u5220\u9664\u5217-1",level:2},{value:"\u6570\u636e\u51c6\u5907",id:"\u6570\u636e\u51c6\u5907-4",level:3},{value:"\u5220\u9664\u4e00\u5217",id:"\u5220\u9664\u4e00\u5217",level:3},{value:"\u5220\u9664\u591a\u5217",id:"\u5220\u9664\u591a\u5217",level:3}],d={toc:m},u="wrapper";function p(e){let{components:a,...n}=e;return(0,l.kt)(u,(0,t.Z)({},d,n,{components:a,mdxType:"MDXLayout"}),(0,l.kt)("h2",{id:"\u521b\u5efa-dataframe"},"\u521b\u5efa DataFrame"),(0,l.kt)("p",null,"\u5728 ",(0,l.kt)("strong",{parentName:"p"},"Spark")," \u4e2d\uff0c\u521b\u5efa ",(0,l.kt)("strong",{parentName:"p"},"DataFrame")," \u7684\u4e3b\u8981\u65b9\u6cd5\u662f ",(0,l.kt)("inlineCode",{parentName:"p"},"createDataFrame()")," \u548c ",(0,l.kt)("inlineCode",{parentName:"p"},"toDF()")," \u3002"),(0,l.kt)("h3",{id:"\u6570\u636e\u51c6\u5907"},"\u6570\u636e\u51c6\u5907"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")\nval spark = SparkSession.builder().config(sparkConf).getOrCreate()\nimport spark.implicits._\n\nval columns = Seq("language","users_count")\nval data = Seq(("Java", "20000"), ("Python", "100000"), ("Scala", "3000"))\nval rdd = spark.sparkContext.parallelize(data)\n')),(0,l.kt)("h3",{id:"\u901a\u8fc7-rdd-\u521b\u5efa-dataframe"},"\u901a\u8fc7 RDD \u521b\u5efa DataFrame"),(0,l.kt)("h4",{id:"todf"},"toDF()"),(0,l.kt)("p",null,"\u901a\u8fc7 ",(0,l.kt)("inlineCode",{parentName:"p"},"toDF()")," \u65b9\u6cd5\uff0c\u53ef\u4ee5\u76f4\u63a5\u5c06 RDD \u8f6c\u6362\u4e3a DataFrame\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u4f1a\u5c06\u5217\u547d\u540d\u4e3a ",(0,l.kt)("strong",{parentName:"p"},"\u201c","_","1\u201d")," \u548c ",(0,l.kt)("strong",{parentName:"p"},"\u201c","_","2\u201d")," \u3002\uff08\u6b64\u5904\u6709\u4e24\u5217\uff09"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"val dfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n")),(0,l.kt)("p",null,"\u6b64\u5916\uff0c",(0,l.kt)("strong",{parentName:"p"},"toDF()")," \u8fd8\u53ef\u4ee5\u76f4\u63a5\u6307\u5b9a\u5217\u540d\uff1a"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val dfFromRDD1 = rdd.toDF("language","users_count")\ndfFromRDD1.printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n")),(0,l.kt)("admonition",{type:"tip"},(0,l.kt)("p",{parentName:"admonition"},"\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5217\u7684\u7c7b\u578b\u4e3a ",(0,l.kt)("strong",{parentName:"p"},"String")," \uff1b\u5982\u679c\u9700\u8981\u6307\u5b9a\u5217\u7684\u6570\u636e\u7c7b\u578b\uff0c\u5219\u9700\u8981\u4f7f\u7528 ",(0,l.kt)("strong",{parentName:"p"},"Schema")," \u3002")),(0,l.kt)("h4",{id:"createdataframe"},"createDataFrame()"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)\n")),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"createDataFrame()")," \u65b9\u6cd5\u652f\u6301 ",(0,l.kt)("strong",{parentName:"p"},"RDD","[Row]")," \u6570\u636e\u7c7b\u578b\uff0c\u5728\u4f7f\u7528\u7684\u65f6\u5019\u9700\u8981\u5b9a\u4e49 ",(0,l.kt)("strong",{parentName:"p"},"Schema"),"\uff1a"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.spark.sql.types.{StringType, StructField, StructType}\nimport org.apache.spark.sql.Row\n\nval schema = StructType(Array(\n    StructField("language", StringType, nullable = true),\n    StructField("users_count", StringType, nullable = true)\n))\nval rowRDD = rdd.map(attributes => Row(attributes._1, attributes._2))\nval dfFromRDD3 = spark.createDataFrame(rowRDD, schema)\n')),(0,l.kt)("h3",{id:"\u901a\u8fc7-list-\u548c-seq-\u96c6\u5408\u521b\u5efa-dataframe"},"\u901a\u8fc7 List \u548c Seq \u96c6\u5408\u521b\u5efa DataFrame"),(0,l.kt)("h4",{id:"todf-1"},"toDF()"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"import spark.implicits._\nval dfFromData1 = data.toDF() \n")),(0,l.kt)("h4",{id:"createdataframe-1"},"createDataFrame()"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"//From Data (USING createDataFrame)\nvar dfFromData2 = spark.createDataFrame(data).toDF(columns:_*)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'import scala.collection.JavaConversions._\n//From Data (USING createDataFrame and Adding schema using StructType)\nval schema = StructType(Array(\n    StructField("language", StringType, nullable = true),\n    StructField("users_count", StringType, nullable = true)\n))\nval rowData= Seq(Row("Java", "20000"), \n               Row("Python", "100000"), \n               Row("Scala", "3000"))\nvar dfFromData3 = spark.createDataFrame(rowData,schema)\n')),(0,l.kt)("h3",{id:"\u901a\u8fc7-csv-\u521b\u5efa-dataframe"},"\u901a\u8fc7 CSV \u521b\u5efa DataFrame"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val df2 = spark.read.csv("/src/resources/file.csv")\n')),(0,l.kt)("h3",{id:"\u901a\u8fc7-txt-\u521b\u5efa-dataframe"},"\u901a\u8fc7 TXT \u521b\u5efa DataFrame"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val df2 = spark.read.text("/src/resources/file.txt")\n')),(0,l.kt)("h3",{id:"\u901a\u8fc7-json-\u521b\u5efa-dataframe"},"\u901a\u8fc7 JSON \u521b\u5efa DataFrame"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val df2 = spark.read.json("/src/resources/file.json")\n')),(0,l.kt)("h3",{id:"\u901a\u8fc7-xml-\u521b\u5efa-dataframe"},"\u901a\u8fc7 XML \u521b\u5efa DataFrame"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n     <groupId>com.databricks</groupId>\n     <artifactId>spark-xml_2.11</artifactId>\n     <version>0.6.0</version>\n </dependency>\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val df = spark.read\n      .format("com.databricks.spark.xml")\n      .option("rowTag", "person")\n      .xml("src/main/resources/persons.xml")\n')),(0,l.kt)("h3",{id:"\u901a\u8fc7-mysql-\u521b\u5efa-dataframe"},"\u901a\u8fc7 MySQL \u521b\u5efa DataFrame"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"val df_mysql = spark.read.format(\u201cjdbc\u201d)\n   .option(\u201curl\u201d, \u201cjdbc:mysql://localhost:port/db\u201d)\n   .option(\u201cdriver\u201d, \u201ccom.mysql.jdbc.Driver\u201d)\n   .option(\u201cdbtable\u201d, \u201ctablename\u201d) \n   .option(\u201cuser\u201d, \u201cuser\u201d) \n   .option(\u201cpassword\u201d, \u201cpassword\u201d) \n   .load()\n")),(0,l.kt)("h2",{id:"\u9009\u62e9\u5217"},"\u9009\u62e9\u5217"),(0,l.kt)("h3",{id:"\u6570\u636e\u51c6\u5907-1"},"\u6570\u636e\u51c6\u5907"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val spark = SparkSession.builder().config(sparkConf).getOrCreate()\nimport spark.implicits._\n\nval data = Seq(("James", "Smith", "USA", "CA"),\n      ("Michael", "Rose", "USA", "NY"),\n      ("Robert", "Williams", "USA", "CA"),\n      ("Maria", "Jones", "USA", "FL"))\n\nval columns = Seq("firstname", "lastname", "country", "state")\n\nval df = data.toDF(columns: _*)\ndf.show(false)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n")),(0,l.kt)("h3",{id:"\u9009\u62e9\u5355\u5217\u6216\u591a\u5217"},"\u9009\u62e9\u5355\u5217\u6216\u591a\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'df.select("firstname", "lastname").show()\n\ndf.select(df("firstname"), df("lastname")).show()\n\n//Using col function, use alias() to get alias name\nimport org.apache.spark.sql.functions.col\ndf.select(col("firstname").alias("fname"), col("lastname")).show()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n\n+-------+--------+\n|  fname|lastname|\n+-------+--------+\n|  James|   Smith|\n|Michael|    Rose|\n| Robert|Williams|\n|  Maria|   Jones|\n+-------+--------+\n")),(0,l.kt)("h3",{id:"\u9009\u62e9\u6240\u6709\u5217"},"\u9009\u62e9\u6240\u6709\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'//Show all columns from DataFrame\ndf.select("*").show()\nval columnsAll = df.columns.map(m => col(m))\ndf.select(columnsAll: _*).show()\ndf.select(columns.map(m => col(m)): _*).show()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n")),(0,l.kt)("h3",{id:"\u6839\u636e\u5217\u8868\u9009\u62e9\u5217"},"\u6839\u636e\u5217\u8868\u9009\u62e9\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val listCols = List("lastname", "country")\ndf.select(listCols.map(m => col(m)): _*).show()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+--------+-------+\n|lastname|country|\n+--------+-------+\n|   Smith|    USA|\n|    Rose|    USA|\n|Williams|    USA|\n|   Jones|    USA|\n+--------+-------+\n")),(0,l.kt)("h3",{id:"\u9009\u62e9\u524d-n-\u5217"},"\u9009\u62e9\u524d N \u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"//Select first 3 columns.\ndf.select(df.columns.slice(0, 3).map(m => col(m)): _*).show()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------+--------+-------+\n|firstname|lastname|country|\n+---------+--------+-------+\n|    James|   Smith|    USA|\n|  Michael|    Rose|    USA|\n|   Robert|Williams|    USA|\n|    Maria|   Jones|    USA|\n+---------+--------+-------+\n")),(0,l.kt)("h3",{id:"\u6839\u636e\u4f4d\u7f6e\u6216\u7d22\u5f15\u9009\u62e9\u5217"},"\u6839\u636e\u4f4d\u7f6e\u6216\u7d22\u5f15\u9009\u62e9\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"//Selects 4th column (index starts from zero)\ndf.select(df.columns(3)).show()\n//Selects columns from index 2 to 4\ndf.select(df.columns.slice(2, 4).map(m => col(m)): _*).show()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+-----+\n|state|\n+-----+\n|   CA|\n|   NY|\n|   CA|\n|   FL|\n+-----+\n\n+-------+-----+\n|country|state|\n+-------+-----+\n|    USA|   CA|\n|    USA|   NY|\n|    USA|   CA|\n|    USA|   FL|\n+-------+-----+\n")),(0,l.kt)("h3",{id:"\u901a\u8fc7\u6b63\u5219\u9009\u62e9\u5217"},"\u901a\u8fc7\u6b63\u5219\u9009\u62e9\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'//Select columns by regular expression\ndf.select(df.colRegex("`^.*name*`")).show()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n")),(0,l.kt)("h3",{id:"\u6839\u636e-starts-\u6216\u8005-ends-\u9009\u62e9\u5217"},"\u6839\u636e starts \u6216\u8005 ends \u9009\u62e9\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'df.select(df.columns.filter(f => f.startsWith("first")).map(m => col(m)): _*)\ndf.select(df.columns.filter(f => f.endsWith("name")).map(m => col(m)): _*)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------+\n|firstname|\n+---------+\n|    James|\n|  Michael|\n|   Robert|\n|    Maria|\n+---------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n")),(0,l.kt)("h3",{id:"\u9009\u62e9\u591a\u5c42\u7ea7\u5217"},"\u9009\u62e9\u591a\u5c42\u7ea7\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'//Show Nested columns\nval data2 = Seq(Row(Row("James", "", "Smith"), "OH", "M"),\n    Row(Row("Anna", "Rose", ""), "NY", "F"),\n    Row(Row("Julia", "", "Williams"), "OH", "F"),\n    Row(Row("Maria", "Anne", "Jones"), "NY", "M"),\n    Row(Row("Jen", "Mary", "Brown"), "NY", "M"),\n    Row(Row("Mike", "Mary", "Williams"), "OH", "M")\n)\n\nval schema = new StructType()\n    .add("name", new StructType()\n        .add("firstname", StringType)\n        .add("middlename", StringType)\n        .add("lastname", StringType))\n    .add("state", StringType)\n    .add("gender", StringType)\n\nval df2 = spark.createDataFrame(spark.sparkContext.parallelize(data2), schema)\ndf2.printSchema()\ndf2.show(false)\ndf2.select("name").show(false)\ndf2.select("name.firstname", "name.lastname").show(false)\ndf2.select("name.*").show(false)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+----------------------+-----+------+\n|name                  |state|gender|\n+----------------------+-----+------+\n|[James, , Smith]      |OH   |M     |\n|[Anna, Rose, ]        |NY   |F     |\n|[Julia, , Williams]   |OH   |F     |\n|[Maria, Anne, Jones]  |NY   |M     |\n|[Jen, Mary, Brown]    |NY   |M     |\n|[Mike, Mary, Williams]|OH   |M     |\n+----------------------+-----+------+\n\n+----------------------+\n|name                  |\n+----------------------+\n|[James, , Smith]      |\n|[Anna, Rose, ]        |\n|[Julia, , Williams]   |\n|[Maria, Anne, Jones]  |\n|[Jen, Mary, Brown]    |\n|[Mike, Mary, Williams]|\n+----------------------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|James    |Smith   |\n|Anna     |        |\n|Julia    |Williams|\n|Maria    |Jones   |\n|Jen      |Brown   |\n|Mike     |Williams|\n+---------+--------+\n\n+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n|James    |          |Smith   |\n|Anna     |Rose      |        |\n|Julia    |          |Williams|\n|Maria    |Anne      |Jones   |\n|Jen      |Mary      |Brown   |\n|Mike     |Mary      |Williams|\n+---------+----------+--------+\n")),(0,l.kt)("h2",{id:"\u521b\u5efa\u6216\u4fee\u6539\u5217"},"\u521b\u5efa\u6216\u4fee\u6539\u5217"),(0,l.kt)("h3",{id:"\u6570\u636e\u51c6\u5907-2"},"\u6570\u636e\u51c6\u5907"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val arrayStructureData = Seq(\n    Row(Row("James ", "", "Smith"), "1", "M", 3100, List("Cricket", "Movies"), Map("hair" -> "black", "eye" -> "brown")),\n    Row(Row("Michael ", "Rose", ""), "2", "M", 3100, List("Tennis"), Map("hair" -> "brown", "eye" -> "black")),\n    Row(Row("Robert ", "", "Williams"), "3", "M", 3100, List("Cooking", "Football"), Map("hair" -> "red", "eye" -> "gray")),\n    Row(Row("Maria ", "Anne", "Jones"), "4", "M", 3100, null, Map("hair" -> "blond", "eye" -> "red")),\n    Row(Row("Jen", "Mary", "Brown"), "5", "M", 3100, List("Blogging"), Map("white" -> "black", "eye" -> "black"))\n    )\n\nval arrayStructureSchema = new StructType()\n    .add("name", new StructType()\n        .add("firstname", StringType)\n        .add("middlename", StringType)\n        .add("lastname", StringType))\n    .add("id", StringType)\n    .add("gender", StringType)\n    .add("salary", IntegerType)\n    .add("Hobbies", ArrayType(StringType))\n    .add("properties", MapType(StringType, StringType))\n\nval df = spark.createDataFrame(\n      spark.sparkContext.parallelize(arrayStructureData), arrayStructureSchema)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------------------+---+------+------+-------------------+------------------------------+\n|name                 |id |gender|salary|Hobbies            |properties                    |\n+---------------------+---+------+------+-------------------+------------------------------+\n|[James , , Smith]    |1  |M     |3100  |[Cricket, Movies]  |[hair -> black, eye -> brown] |\n|[Michael , Rose, ]   |2  |M     |3100  |[Tennis]           |[hair -> brown, eye -> black] |\n|[Robert , , Williams]|3  |M     |3100  |[Cooking, Football]|[hair -> red, eye -> gray]    |\n|[Maria , Anne, Jones]|4  |M     |3100  |null               |[hair -> blond, eye -> red]   |\n|[Jen, Mary, Brown]   |5  |M     |3100  |[Blogging]         |[white -> black, eye -> black]|\n+---------------------+---+------+------+-------------------+------------------------------+\n")),(0,l.kt)("h3",{id:"\u521b\u5efa\u65b0\u5217"},"\u521b\u5efa\u65b0\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.spark.sql.functions.lit\ndf.withColumn("Country", lit("USA")).show(false)\n\n//chaining to operate on multiple columns\ndf.withColumn("Country", lit("USA"))\n    .withColumn("anotherColumn",lit("anotherValue")).show(false)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------------------+---+------+------+-------------------+------------------------------+-------+\n|name                 |id |gender|salary|Hobbies            |properties                    |Country|\n+---------------------+---+------+------+-------------------+------------------------------+-------+\n|[James , , Smith]    |1  |M     |3100  |[Cricket, Movies]  |[hair -> black, eye -> brown] |USA    |\n|[Michael , Rose, ]   |2  |M     |3100  |[Tennis]           |[hair -> brown, eye -> black] |USA    |\n|[Robert , , Williams]|3  |M     |3100  |[Cooking, Football]|[hair -> red, eye -> gray]    |USA    |\n|[Maria , Anne, Jones]|4  |M     |3100  |null               |[hair -> blond, eye -> red]   |USA    |\n|[Jen, Mary, Brown]   |5  |M     |3100  |[Blogging]         |[white -> black, eye -> black]|USA    |\n+---------------------+---+------+------+-------------------+------------------------------+-------+\n\n+---------------------+---+------+------+-------------------+------------------------------+-------+-------------+\n|name                 |id |gender|salary|Hobbies            |properties                    |Country|anotherColumn|\n+---------------------+---+------+------+-------------------+------------------------------+-------+-------------+\n|[James , , Smith]    |1  |M     |3100  |[Cricket, Movies]  |[hair -> black, eye -> brown] |USA    |anotherValue |\n|[Michael , Rose, ]   |2  |M     |3100  |[Tennis]           |[hair -> brown, eye -> black] |USA    |anotherValue |\n|[Robert , , Williams]|3  |M     |3100  |[Cooking, Football]|[hair -> red, eye -> gray]    |USA    |anotherValue |\n|[Maria , Anne, Jones]|4  |M     |3100  |null               |[hair -> blond, eye -> red]   |USA    |anotherValue |\n|[Jen, Mary, Brown]   |5  |M     |3100  |[Blogging]         |[white -> black, eye -> black]|USA    |anotherValue |\n+---------------------+---+------+------+-------------------+------------------------------+-------+-------------+\n")),(0,l.kt)("admonition",{type:"tip"},(0,l.kt)("p",{parentName:"admonition"},(0,l.kt)("inlineCode",{parentName:"p"},"lit()")," \u51fd\u6570\u7528\u4e8e\u5c06\u5e38\u91cf\u8f6c\u6362\u4e3a ",(0,l.kt)("strong",{parentName:"p"},"DataFrame")," \u5217\u7684\u503c\u3002")),(0,l.kt)("admonition",{type:"caution"},(0,l.kt)("p",{parentName:"admonition"},(0,l.kt)("inlineCode",{parentName:"p"},"withColumn()")," \u53ea\u9002\u5408\u64cd\u4f5c\u8f83\u5c11\u7684\u5217\uff0c\u5426\u5219\u4f1a\u6709\u6267\u884c\u6548\u7387\u95ee\u9898\u3002")),(0,l.kt)("h3",{id:"\u4fee\u6539\u5217\u7684\u503c"},"\u4fee\u6539\u5217\u7684\u503c"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.spark.sql.functions.col\ndf.withColumn("salary", col("salary") * 100).show(false)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------------------+---+------+------+-------------------+------------------------------+\n|name                 |id |gender|salary|Hobbies            |properties                    |\n+---------------------+---+------+------+-------------------+------------------------------+\n|[James , , Smith]    |1  |M     |310000|[Cricket, Movies]  |[hair -> black, eye -> brown] |\n|[Michael , Rose, ]   |2  |M     |310000|[Tennis]           |[hair -> brown, eye -> black] |\n|[Robert , , Williams]|3  |M     |310000|[Cooking, Football]|[hair -> red, eye -> gray]    |\n|[Maria , Anne, Jones]|4  |M     |310000|null               |[hair -> blond, eye -> red]   |\n|[Jen, Mary, Brown]   |5  |M     |310000|[Blogging]         |[white -> black, eye -> black]|\n+---------------------+---+------+------+-------------------+------------------------------+\n")),(0,l.kt)("h3",{id:"\u6839\u636e\u73b0\u6709\u5217\u589e\u52a0\u65b0\u5217"},"\u6839\u636e\u73b0\u6709\u5217\u589e\u52a0\u65b0\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.spark.sql.functions.col\ndf.withColumn("CopiedColumn", col("salary") * -1).show(false)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------------------+---+------+------+-------------------+------------------------------+------------+\n|name                 |id |gender|salary|Hobbies            |properties                    |CopiedColumn|\n+---------------------+---+------+------+-------------------+------------------------------+------------+\n|[James , , Smith]    |1  |M     |3100  |[Cricket, Movies]  |[hair -> black, eye -> brown] |-3100       |\n|[Michael , Rose, ]   |2  |M     |3100  |[Tennis]           |[hair -> brown, eye -> black] |-3100       |\n|[Robert , , Williams]|3  |M     |3100  |[Cooking, Football]|[hair -> red, eye -> gray]    |-3100       |\n|[Maria , Anne, Jones]|4  |M     |3100  |null               |[hair -> blond, eye -> red]   |-3100       |\n|[Jen, Mary, Brown]   |5  |M     |3100  |[Blogging]         |[white -> black, eye -> black]|-3100       |\n+---------------------+---+------+------+-------------------+------------------------------+------------+\n")),(0,l.kt)("h3",{id:"\u4fee\u6539\u5217\u7c7b\u578b"},"\u4fee\u6539\u5217\u7c7b\u578b"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.spark.sql.functions.col\nvar df1 = df.withColumn("salary", col("salary").cast("Integer"))\ndf1.printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- Hobbies: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n")),(0,l.kt)("h3",{id:"\u4fee\u6539\u591a\u5217"},"\u4fee\u6539\u591a\u5217"),(0,l.kt)("p",null,"\u5728 ",(0,l.kt)("strong",{parentName:"p"},"Spark DataFrame")," \u4e2d\uff0c\u6d89\u53ca\u4fee\u6539\u591a\u5217\u7684\u64cd\u4f5c\u4e00\u822c\u4e0d\u4f7f\u7528 ",(0,l.kt)("inlineCode",{parentName:"p"},"withColumn()")," \u65b9\u6cd5\uff0c\u63a8\u8350\u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u89c6\u56fe\uff0c\u7136\u540e\u4f7f\u7528 ",(0,l.kt)("inlineCode",{parentName:"p"},"Select()")," \u65b9\u6cd5\u5bf9 ",(0,l.kt)("strong",{parentName:"p"},"DataFrame")," \u8fdb\u884c\u4fee\u6539\u3002"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'df.createOrReplaceTempView("PERSON")\nspark.sql(\n    """SELECT salary*100 as salary, \n      |        salary*-1 as CopiedColumn,\n      |            \'USA\' as country FROM PERSON""".stripMargin).show()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+------+------------+-------+\n|salary|CopiedColumn|country|\n+------+------------+-------+\n|310000|       -3100|    USA|\n|310000|       -3100|    USA|\n|310000|       -3100|    USA|\n|310000|       -3100|    USA|\n|310000|       -3100|    USA|\n+------+------------+-------+\n")),(0,l.kt)("h3",{id:"\u5217\u91cd\u547d\u540d"},"\u5217\u91cd\u547d\u540d"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'df.withColumnRenamed("gender", "sex").show(false)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"+---------------------+---+---+------+-------------------+------------------------------+\n|name                 |id |sex|salary|Hobbies            |properties                    |\n+---------------------+---+---+------+-------------------+------------------------------+\n|[James , , Smith]    |1  |M  |3100  |[Cricket, Movies]  |[hair -> black, eye -> brown] |\n|[Michael , Rose, ]   |2  |M  |3100  |[Tennis]           |[hair -> brown, eye -> black] |\n|[Robert , , Williams]|3  |M  |3100  |[Cooking, Football]|[hair -> red, eye -> gray]    |\n|[Maria , Anne, Jones]|4  |M  |3100  |null               |[hair -> blond, eye -> red]   |\n|[Jen, Mary, Brown]   |5  |M  |3100  |[Blogging]         |[white -> black, eye -> black]|\n+---------------------+---+---+------+-------------------+------------------------------+\n")),(0,l.kt)("h3",{id:"\u5220\u9664\u5217"},"\u5220\u9664\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'df.drop("CopiedColumn")\n')),(0,l.kt)("h3",{id:"\u62c6\u5206\u5217\u4e3a\u591a\u5217"},"\u62c6\u5206\u5217\u4e3a\u591a\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val columns = Seq("name", "address")\nval data = Seq(("Robert, Smith", "1 Main st, Newark, NJ, 92537"),\n    ("Maria, Garcia", "3456 Walnut st, Newark, NJ, 94732"))\nvar dfFromData = spark.createDataFrame(data).toDF(columns: _*)\ndfFromData.printSchema()\n\nval newDF = dfFromData.map(f => {\n    val nameSplit = f.getAs[String](0).split(",")\n    val addSplit = f.getAs[String](1).split(",")\n    (nameSplit(0), nameSplit(1), addSplit(0), addSplit(1), addSplit(2), addSplit(3))\n})\nval finalDF = newDF.toDF("First Name", "Last Name",\n    "Address Line1", "City", "State", "zipCode")\nfinalDF.printSchema()\nfinalDF.show(false)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- name: string (nullable = true)\n |-- address: string (nullable = true)\n\nroot\n |-- First Name: string (nullable = true)\n |-- Last Name: string (nullable = true)\n |-- Address Line1: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- zipCode: string (nullable = true)\n\n+----------+---------+--------------+-------+-----+-------+\n|First Name|Last Name|Address Line1 |City   |State|zipCode|\n+----------+---------+--------------+-------+-----+-------+\n|Robert    | Smith   |1 Main st     | Newark| NJ  | 92537 |\n|Maria     | Garcia  |3456 Walnut st| Newark| NJ  | 94732 |\n+----------+---------+--------------+-------+-----+-------+\n")),(0,l.kt)("h2",{id:"\u5217\u91cd\u547d\u540d-1"},"\u5217\u91cd\u547d\u540d"),(0,l.kt)("h3",{id:"\u6570\u636e\u51c6\u5907-3"},"\u6570\u636e\u51c6\u5907"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val data = Seq(Row(Row("James ", "", "Smith"), "36636", "M", 3000),\n    Row(Row("Michael ", "Rose", ""), "40288", "M", 4000),\n    Row(Row("Robert ", "", "Williams"), "42114", "M", 4000),\n    Row(Row("Maria ", "Anne", "Jones"), "39192", "F", 4000),\n    Row(Row("Jen", "Mary", "Brown"), "", "F", -1)\n)\n    \nval schema = new StructType()\n    .add("name", new StructType()\n        .add("firstname", StringType)\n        .add("middlename", StringType)\n        .add("lastname", StringType))\n    .add("dob", StringType)\n    .add("gender", StringType)\n    .add("salary", IntegerType)\n\nval df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n    df.printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n")),(0,l.kt)("h3",{id:"\u57fa\u672c\u4f7f\u7528"},"\u57fa\u672c\u4f7f\u7528"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'df.withColumnRenamed("dob", "DateOfBirth")\n    .printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n")),(0,l.kt)("h3",{id:"\u91cd\u547d\u540d\u591a\u5217"},"\u91cd\u547d\u540d\u591a\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val df2 = df.withColumnRenamed("dob", "DateOfBirth")\n    .withColumnRenamed("salary", "salary_amount")\ndf2.printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary_amount: integer (nullable = true)\n")),(0,l.kt)("h3",{id:"\u4f7f\u7528-spark-structtype-\u91cd\u547d\u540d"},"\u4f7f\u7528 Spark StructType \u91cd\u547d\u540d"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val schema2 = new StructType()\n    .add("fname", StringType)\n    .add("middlename", StringType)\n    .add("lname", StringType)\n\ndf.select(col("name").cast(schema2),\n    col("dob"),\n    col("gender"),\n    col("salary"))\n    .printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n")),(0,l.kt)("h3",{id:"\u4f7f\u7528-select-\u91cd\u547d\u540d"},"\u4f7f\u7528 Select \u91cd\u547d\u540d"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'df.select(col("name.firstname").as("fname"),\n    col("name.middlename").as("mname"),\n    col("name.lastname").as("lname"),\n    col("dob"), col("gender"), col("salary"))\n    .printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n")),(0,l.kt)("h3",{id:"\u4f7f\u7528-withcolumn-\u91cd\u547d\u540d"},"\u4f7f\u7528 withColumn \u91cd\u547d\u540d"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val df4 = df.withColumn("fname", col("name.firstname"))\n    .withColumn("mname", col("name.middlename"))\n    .withColumn("lname", col("name.lastname"))\n    .drop("name")\ndf4.printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n")),(0,l.kt)("h3",{id:"\u4f7f\u7528-col-\u6279\u91cf\u4fee\u6539\u5217\u540d"},"\u4f7f\u7528 col() \u6279\u91cf\u4fee\u6539\u5217\u540d"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val old_columns = Seq("dob","gender","salary","fname","mname","lname")\nval new_columns = Seq("DateOfBirth","Sex","salary","firstName","middleName","lastName")\nval columnsList = old_columns.zip(new_columns).map(f=>{col(f._1).as(f._2)})\nval df5 = df4.select(columnsList:_*)\ndf5.printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- DateOfBirth: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- firstName: string (nullable = true)\n |-- middleName: string (nullable = true)\n |-- lastName: string (nullable = true)\n")),(0,l.kt)("h2",{id:"\u5220\u9664\u5217-1"},"\u5220\u9664\u5217"),(0,l.kt)("h3",{id:"\u6570\u636e\u51c6\u5907-4"},"\u6570\u636e\u51c6\u5907"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val structureData = Seq(\n    Row("James", "", "Smith", "36636", "NewYork", 3100),\n    Row("Michael", "Rose", "", "40288", "California", 4300),\n    Row("Robert", "", "Williams", "42114", "Florida", 1400),\n    Row("Maria", "Anne", "Jones", "39192", "Florida", 5500),\n    Row("Jen", "Mary", "Brown", "34561", "NewYork", 3000)\n)\n\nval structureSchema = new StructType()\n    .add("firstname", StringType)\n    .add("middlename", StringType)\n    .add("lastname", StringType)\n    .add("id", StringType)\n    .add("location", StringType)\n    .add("salary", IntegerType)\n\nval df = spark.createDataFrame(\n    spark.sparkContext.parallelize(structureData), structureSchema)\ndf.printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: integer (nullable = true)\n")),(0,l.kt)("h3",{id:"\u5220\u9664\u4e00\u5217"},"\u5220\u9664\u4e00\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'val df2 = df.drop("firstname") //First signature\ndf2.printSchema()\n\ndf.drop(df("firstname")).printSchema()\n//import org.apache.spark.sql.functions.col is required\ndf.drop(col("firstname")).printSchema() //Third signature\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: integer (nullable = true)\n")),(0,l.kt)("h3",{id:"\u5220\u9664\u591a\u5217"},"\u5220\u9664\u591a\u5217"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'//Refering more than one column\ndf.drop("firstname", "middlename", "lastname")\n    .printSchema()\n\n// using array/sequence of columns\nval cols = Seq("firstname", "middlename", "lastname")\ndf.drop(cols: _*)\n    .printSchema()\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"root\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: integer (nullable = true)\n")))}p.isMDXComponent=!0}}]);